---
title: "DEM 7223 - Event History Analysis - Comparing Survival Times Between Groups"
author:
- name: '[Corey S. Sparks, PhD](https://coreysparks.github.io)'
  affiliation: '[The University of Texas at San Antonio](https://hcap.utsa.edu/demography)'
date: "August 18, 2020"
output:
  pdf_document:
    toc: yes
  html_document:
    df_print: paged
    fig_height: 7
    fig_width: 7
    toc: yes
    toc_float: yes
---

## Product Limit Estimation

[Kaplan and Meier (1958)](https://www.tandfonline.com/doi/pdf/10.1080/01621459.1958.10501452?casa_token=YIcJDiyQjYwAAAAA:qTa-OQEPtvVy6p4QoBCMx1VIgMey7tJWq-21zMj0LzYKFbKkRO_MYvO1V_7f8qihpgbgllZAa65O) derived an estimator of the survivorship function for a sample of censored and uncensored cases. 

* The method figured survival to a time, t, is the product of the survival from all previous time points.

* i.e. you can only get to time 3, if you survive time 1 and time 2, etc

We can write this as

$$\hat{S(t)} = (1-\hat{p(t_1)})(1-\hat{p(t_2)}) \dots (1-\hat{p(t_j)})$$

Unlike the life-table and discrete time methods of estimating survival, which lumped time into discrete periods, K-M uses the information contained in the actual duration.

Each K-M interval begins with a single event time, and ends just prior to the next event time.

### Kaplan-Meier Estimation
The K-M estimator is written:
$$\hat{S(t)} = \prod_{t_i \leqslant t}\frac{n_i - d_i}{n_i} = \prod_{t_i \leqslant t} \left[1- \frac{d_i}{Y_i} \right]$$

Where the $t_i$ are the ranked survival times, $n_i$ is the number of individuals at risk at each time, $t_i$ is time, $d_i$ is the number of events at each time. 

When censoring is present, you define $n_i$ by subtracting out the number of censored cases at that particular time.
```{r, echo=FALSE, message=FALSE}
library(knitr)
library(kableExtra)
library(dplyr)
library(ggplot2)
```

```{r, results='asis'}
t1<-data.frame(Time_Interval=c("[0,1)", "[1,2)", "[2,4)", "[4, 5)", "[5, 6)", "[6+)"),
               time=c(0, 1, 2, 4, 5, 6),
               n=c(100,NA, NA, NA, NA, NA),
               d=c(15, 5, 1, 2, 5, 2),
               c=c(0,2,5,2,0,2),
               prob=c(1,NA, NA, NA, NA, NA), 
               St=c(1,NA, NA, NA, NA, NA))

for (i in 2:6){
t1$n[i]<-t1$n[i-1]- t1$d[i-1] -t1$c[i-1]
t1$prob[i]<-1-(t1$d[i]/t1$n[i])
}

t1$St<-cumprod(t1$prob)

t1%>%
  kable(format ="simple" )#%>%
#  column_spec(1:4, border_left = T, border_right = T)#%>%
#  kable_styling()

t1%>%
  ggplot()+
  geom_step(aes(x=time, y=St,color="a"))+
  geom_step(aes(x=time, y=prob, color="b"))+
  scale_colour_identity(guide="legend")+
  scale_colour_manual(name = 'Function Type', 
                      values =c('a'='red','b'='green'),
         labels = c('S(t)','p(t)'))+
  xlab("Time")+ylab("Probability")+
  ggtitle("Kaplan - Meier Survival and Probability Functions")
```

### K-M and the hazard function
The exact estimate of the K-M hazard function actually depends on the width of the time interval at each observed time point.

$$\hat{h(t_j)} = \frac{\hat{p_{KM}(t_j)}}{\text{width}_j}$$
and you can get this estimate from the `muhaz` library in R. 

### Variance in the K-M Estimates
Since the K-M survival function is a statistical estimate, it also has uncertainty to it. To measure the uncertainty, or variance in the estimate, the traditional method is to use the Greenwood formula.

$$\text{Var}(S(t)) = \hat{S(t)}^2 \sum_{t_i \leqslant t} \frac{d_i}{n_i(n_i-d_i)}$$
and standard error equal to $s.e.(S(t))= \sqrt{\text{Var}(S(t))}$

If we have a standard error of the survival function, the assuming the sampling distribution of the survival function is normal, we can calculate a normal confidence interval for the survival function at each time:

$$c.i.(S(t)) = \hat{S(t)} \pm z_{1-\alpha/2} * s.e.(S(t))$$

where $z$ is the standard normal variate corresponding to the the $1-\alpha/2$ level of confidence.

#### Estimating the cumulative hazard function.
If we have estimates of $\hat{h(t_j)}$, we can either calculate the value of the cumulative hazard function using the relationship among survival function:

$$H(t) = -\text{log } S(t)$$
or use the Nelson-Aalen estimator:

$$\hat{H(t)} = \sum_{t_i \leqslant t} \frac{d_i}{Y_i}$$

## Comparing Survival curves between groups

Often in our data there are distinct groups for which we want to compare survival patterns
* e.g. treatment vs. control group in clinical trial
* e.g. proportion of women without a first birth by education category

To do this we typically construct a variable that represents an identifier for members of reach group. This can be referred to as an indicator, or class, variable. 

This is the same process as doing a two sample test for a regular outcome, such as a t-test or chi square test.

* One limitation of survival data is that they are typically skewed, meaning their distribution is not symmetrical

* This means that many traditional hypothesis test (t-test, z-test) for comparison of central tendency are no appropriate

* Instead we use a variety of non-parametric methods 

* Simply meaning that these tests are not dependent on the shape of the distribution or on the parameters of the distribution

* Also, due to *censoring*, traditional distributional parameters like the mean are less meaningful, so tests on said parameters would be incorrect

### Graphical methods of comparison

The first stop on our examination of between group comparison is the *inter-ocular traumatic test*

You may not be familiar with this test, but in general, if you look at a plot, and if you think there's a difference, say between two lines, there usually is, and many times the human eye is a more discerning test than anything.

```{r, echo=FALSE}
x<-rnorm(100, mean=10, sd=1)
x2<-rnorm(100, mean=2, sd=2)
df<-data_frame(x=c(x, x2),group=c(rep(1, 100), rep(2, 100)))
df%>%
  ggplot()+geom_density(aes(x, group=group, color=factor(group)))
```

<!-- ![](https://media.giphy.com/media/GQI382aMVej0k/giphy.gif) -->

 
So, plot the survival curves, with confidence intervals, your eye can usually detect if there is a difference

Under traditional statistics thinking, if the confidence intervals of the two curves overlap for their entire lengths, then the two groups are equivalent, if the confidence interval for the curves do not overlap at ANY point along the curve, they are different, simple, no?

The *statistical* way of doing this, beyond looking at things, is the realm of Mantel-Haenszel test. R implements this test for 2 or *k* groups using the `survdiff()` function. It uses the method of [Harrington and Fleming (1982)](https://www.jstor.org/stable/2335991) which can weight the difference in survival curves flexibly, giving more or less weight to earlier or later survival times. 

The classic Mantel-Haneszel test is just a $\chi^2$ test for independence. 

At each time point, $t_i$, consider the following table for 2 groups:

 ![M-H Test](C:/Users/ozd504/Documents/GitHub/DEM7223/images/mhtable.png)

If we sum the differences between the observed and expected failures across all time points,we arrive with:

Using 1 group as the basis:
$$e_{i1} = \frac{n_{i1}d_{i1}}{n_i}$$
is the expected number of failures. The general form of the test is then:

$$Q = \frac{\sum_{i} w_i (d_{i1} - e_{i1})^2}{\sum_i w_i v_{i1}}$$
Where $v_{i1}$ is the variance in the number of events in group 1. This test follows a $\chi^2$ distribution with 1 degree of freedom.

The value of $w_i$ allows great flexibility for these tests, and is called the weight function at time $i$
This allows the analyst to specify how much you want to weight the difference in survival at a particular time point.

This testing logic also extends to *k-groups*, so instead of doing an ANOVA test, you would do the *k-group* test following this method.

## Example

This example will illustrate how to test for differences between survival functions estimated by the Kaplan-Meier product limit estimator. The tests all follow the methods described by Harrington and Fleming (1982) [Link](http://biomet.oxfordjournals.org/content/69/3/553.short). 

The first example will use as its outcome variable, the event of a child dying before age 1. The data for this example come from the model.data [Demographic and Health Survey for 2012](http://dhsprogram.com/data/dataset/model.dat_Standard-DHS_2012.cfm?flag=0) children's recode file. This file contains information for all births in the last 5 years prior to the survey.

The second example, we will examine how to calculate the survival function for a longitudinally collected data set. Here I use data from the [ECLS-K ](http://nces.ed.gov/ecls/kinderdatainformation.asp). Specifically, we will examine the transition into poverty between kindergarten and fifth grade. 

```{r}
#load libraries
library(haven)
library(survival)
library(car)
library(muhaz)
model.dat<-read_dta("https://github.com/coreysparks/data/blob/master/ZZKR62FL.DTA?raw=true")
model.dat<-zap_labels(model.dat)
```


## Event - Infant Mortality
In the DHS, they record if a child is dead or alive and the age at death if the child is dead. This can be understood using a series of variables about each child. 

If the child is alive at the time of interview, then the variable B5==1, and the age at death is censored. 

If the age at death is censored, then the age at the date of interview (censored age at death) is the date of the interview - date of birth (in months). 

If the child is dead at the time of interview,then the variable B5!=1, then the age at death in months is the variable B7. Here we code this:

```{r}
model.dat$death.age<-ifelse(model.dat$b5==1,
                          ((((model.dat$v008))+1900)-(((model.dat$b3))+1900)) 
                          ,model.dat$b7)

#censoring indicator for death by age 1, in months (12 months)
model.dat$d.event<-ifelse(is.na(model.dat$b7)==T|model.dat$b7>12,0,1)
model.dat$d.eventfac<-factor(model.dat$d.event)
levels(model.dat$d.eventfac)<-c("Alive at 1", "Dead by 1")
table(model.dat$d.eventfac)

```

### Comparing Two Groups
We will now test for differences in survival by characteristics of the household. First we will examine whether the survival chances are the same for children in relatively high ses (in material terms) households, compared to those in relatively low-ses households.

This is the equivalent of doing a t-test, or Mann-Whitney U test for differences between two groups. 

```{r}
library(survminer)
model.dat$highses<-Recode(model.dat$v190, recodes ="1:3 = 0; 4:5=1; else=NA")
fit1<-survfit(Surv(death.age, d.event)~highses, data=model.dat)
fit1

ggsurvplot(fit1, xlim=c(0,12), conf.int=T, 
           title="Survival Function for Infant Mortality - Low vs. High SES Households",
           ylim=c(.8, 1))

summary(fit1)

```

Gives us the basic survival plot. 

Next we will use `survtest()` to test for differences between the two or more groups. The `survdiff()` function performs the log-rank test to compare the survival patterns of two or more groups.

```{r}
#two group compairison
survdiff(Surv(death.age, d.event)~highses, data=model.dat)

```

In this case, we see no difference in survival status based on household SES. 

How about rural vs urban residence?

```{r}
library(dplyr)
library(car)
model.dat<-model.dat%>%
  mutate(rural = car::Recode(v025, recodes ="2 = '0rural'; 1='1urban'; else=NA", as.factor = T))


fit2<-survfit(Surv(death.age, d.event)~rural, data=model.dat, conf.type = "log")
fit2
summary(fit2)

ggsurvplot(fit2, xlim=c(0,12), ylim=c(.8, 1), conf.int=T,
           title="Survival Function for Infant mortality - Rural vs Urban Residence")
```



# Two- sample test

```{r}
survdiff(Surv(death.age, d.event)~rural, data=model.dat)

prop.table(table(model.dat$d.event, model.dat$rural), margin = 2)
chisq.test(table(model.dat$d.event, model.dat$rural))
```
Which shows a statistically significant difference in survival between rural and urban children, with rural children showing lower survivorship at all ages. 


We can also compare the 95% survival point for rural and urban residents
```{r}
quantile(fit2, probs=.05)

```


We can also calculate the hazard function for each group using the `kphaz.fit` function in the `muhaz` library.

```{r}
haz2<-kphaz.fit(model.dat$death.age, model.dat$d.event, model.dat$rural)
haz2
plot(y=haz2$haz[1:12], x=haz2$time[1:12], col=1, lty=1, type="s")
lines(y=haz2$haz[13:24], x=haz2$time[13:24], col=2, lty=1, type="s")

```

 This may be suggestive that children in urban areas may live in poorer environmental conditions.

### k- sample test
Next we illustrate a k-sample test. This would be the equivalent of the ANOVA if we were doing ordinary linear models. 

In this example, I use the `v024` variable, which corresponds to the region of residence in this data. Effectively we are testing for differences in risk of infant mortality by region.

```{r}

table(model.dat$v024, model.dat$d.eventfac)

fit3<-survfit(Surv(death.age, d.event)~v024, data=model.dat)
fit3
#summary(fit3)
#quantile(fit3, probs=.05)

ggsurvplot(fit3,conf.int = T, risk.table = F,
           title = "Survivorship Function for Infant Mortality",
           xlab = "Time in Months", xlim = c(0,12), ylim=c(.8, 1))

survdiff(Surv(death.age, d.event)~v024, data=model.dat)

```
Which shows significant variation in survival between regions. The biggest difference we see is between region 3 green) and region 1 (black line) groups.

Lastly, we examine comparing survival across multiple variables, in this case the education of the mother (`secedu`) and the rural/urban residence `rural`:

```{r}

model.dat<-model.dat%>%
  mutate(secedu=Recode(v106, recodes ="2:3 = 1; 0:1=0; else=NA"))

table(model.dat$secedu, model.dat$d.eventfac)

fit4<-survfit(Surv(death.age, d.event)~rural+secedu, data=model.dat)
#summary(fit4)
ggsurvplot(fit4,conf.int = T, risk.table = F,
           title = "Survivorship Function for Infant Mortality",
           xlab = "Time in Months", xlim = c(0,12), ylim=c(.8, 1))

#plot(fit4, ylim=c(.85,1), xlim=c(0,12), col=c(1,1,2,2),lty=c(1,2,1,2), conf.int=F)
#title(main="Survival Function for Infant Mortality", sub="Rural/Urban * Mother's Education")
#legend("topright", legend = c("Urban, Low Edu","Urban High Edu     ", "Rural, Low Edu","Rural High Edu     " ), col=c(1,1,2,2),lty=c(1,2,1,2))

# test
survdiff(Surv(death.age, d.event)~rural+secedu, data=model.dat)

```

Which shows a marginally significant difference between at *least* two of the groups, in this case, I would say that it's most likely finding differences between the Urban, low Education and the Rural low education, because there have the higher ratio of observed vs expected.


# Survival analysis using survey design

This example will cover the use of R functions for analyzing complex survey data. Most social and health surveys are not simple random samples of the population, but instead consist of respondents from a complex survey design. These designs often stratify the population based on one or more characteristics, including geography, race, age, etc. In addition the designs can be multi-stage, meaning that initial strata are created, then respondents are sampled from smaller units within those strata. An example would be if a school district was chosen as a sample strata, and then schools were then chosen as the primary sampling units (PSUs) within the district. From this 2 stage design, we could further sample classrooms within the school (3 stage design) or simply sample students (or whatever our unit of interest is). 

A second feature of survey data we often want to account for is differential respondent weighting. This means that each respondent is given a weight to represent how common that particular respondent is within the population. This reflects the differenital probability of sampling based on respondent characteristics. As demographers, we are also often interested in making inference for the population, not just the sample, so our results must be generalizable to the population at large. Sample weights are used in the process as well.

When such data are analyzed, we must take into account this nesting structure (sample design) as well as the respondent sample weight in order to make valid estimates of **ANY** statistical parameter. If we do not account for design, the parameter standard errors will be incorrect, and if we do not account for weighting, the parameters themselves will be incorrect and biased. 

In general there are typically three things we need to find in our survey data code books: The sample strata identifier, the sample primary sampling unit identifier (often called a cluster identifier) and the respondent survey weight.   These will typically have one of these names and should be easily identifiable in the code book. 

Statistical software will have special routines for analyzing these types of data and you must be aware that the diversity of statistical routines that generally exists will be lower for analyzing complex survey data, and some forms of analysis *may not be available!*


In the DHS [Recode manual](http://dhsprogram.com/pubs/pdf/DHSG4/Recode6_DHS_22March2013_DHSG4.pdf), the sampling information for the data is found in variables `v021` and `v022`, which are the primary sampling unit (PSU) and sample strata, respectively. The person weight is found in variable `v005`, and following DHS protocol, this has six implied decimal places, so we must divide it by 1000000, again, following the DHS manual.

```{r}
library(survey)
model.dat$wt<-model.dat$v005/1000000

#create the design: ids == PSU, strata==strata, weights==weights.
options(survey.lonely.psu = "adjust")
des<-svydesign(ids=~v021, strata = ~v022, weights=~wt, data=model.dat)

fit.s<-svykm(Surv(death.age, d.event)~rural, design=des, se=T)

#use svyby to find the %of infants that die before age 1, by rural/urban status
svyby(~d.event, ~rural, des, svymean)
```


The plotting is a bit more of a challenge, as the survey version of the function isn't as nice
```{r}
plot(fit.s[[2]], ylim=c(.8,1), xlim=c(0,12),col=1, ci=F )
lines(fit.s[[1]], col=2) 
title(main="Survival Function for Infant Mortality", sub="Rural vs Urban Residence")
legend("topright", legend = c("Urban","Rural" ), col=c(1,2), lty=1)

#test statistic
svylogrank(Surv(death.age, d.event)~rural, design=des)
```

And we see the p-value is larger than assuming random sampling. 

# Using a longitudinal data source
In this example, we will examine how to calculate the survival function for a longitudinally collected data set. Here I use data from the [ECLS-K ](http://nces.ed.gov/ecls/kinderdatainformation.asp). Specifically, we will examine the transition into poverty between kindergarten and third grade. 

First we load our data
```{r}
eclskk5<-readRDS("C:/Users/ozd504/OneDrive - University of Texas at San Antonio/classes/dem7223/dem7223_20//data/eclskk5.rds")
names(eclskk5)<-tolower(names(eclskk5))
#get out only the variables I'm going to use for this example
myvars<-c( "childid","x_chsex_r", "x_raceth_r", "x1kage_r","x4age",
           "x5age", "x6age", "x7age", "x2povty","x4povty_i", "x6povty_i",
           "x8povty_i","x12par1ed_i", "s2_id")
eclskk5<-eclskk5[,myvars]


eclskk5$age1<-ifelse(eclskk5$x1kage_r==-9, NA, eclskk5$x1kage_r/12)
eclskk5$age2<-ifelse(eclskk5$x4age==-9, NA, eclskk5$x4age/12)
#for the later waves, the NCES group the ages into ranges of months,
#so 1= <105 months, 2=105 to 108 months. 
#So, I fix the age at the midpoint of the interval they give,
#and make it into years by dividing by 12

eclskk5$age3<-ifelse(eclskk5$x5age==-9, NA, eclskk5$x5age/12)

eclskk5$pov1<-ifelse(eclskk5$x2povty==1,1,0)
eclskk5$pov2<-ifelse(eclskk5$x4povty_i==1,1,0)
eclskk5$pov3<-ifelse(eclskk5$x6povty_i==1,1,0)

#Recode race with white, non Hispanic as reference using dummy vars
eclskk5$race_rec<-recode (eclskk5$x_raceth_r, recodes="1 = 'nhwhite';2='nhblack';3:4='hispanic';5='nhasian'; 6:8='other';-9=NA", as.factor = T)
eclskk5$male<-recode(eclskk5$x_chsex_r, recodes="1=1; 2=0; -9=NA")
eclskk5$mlths<-recode(eclskk5$x12par1ed_i, recodes = "1:2=1; 3:9=0; else = NA")
eclskk5$mgths<-recode(eclskk5$x12par1ed_i, recodes = "1:3=0; 4:9=1; else =NA") 
```


Now, I need to form the transition variable, this is my event variable, and in this case it will be 1 if a child enters poverty between the first wave of the data and the third grade wave, and 0 otherwise.

**NOTE** I need to remove any children who are already in poverty age wave 1, because they are not at risk of experiencing **this particular** transition. Again, this is called forming the *risk set*

```{r}
eclskk5<-subset(eclskk5, is.na(pov1)==F&is.na(pov2)==F&
                  is.na(pov3)==F&is.na(age1)==F&is.na(age2)==F&
                  is.na(age3)==F&pov1!=1)

eclskk5$povtran1<-ifelse(eclskk5$pov1==0&eclskk5$pov2==0, 0,1)
eclskk5$povtran2<-ifelse(eclskk5$povtran1==1, NA,
                         ifelse(eclskk5$pov2==0&eclskk5$pov3==0,0,1)
                         )

```

Now we do the entire data set. To analyze data longitudinally, we need to reshape the data from the current "wide" format (repeated measures in columns) to a "long" format (repeated observations in rows). The `reshape()` function allows us to do this easily. It allows us to specify our repeated measures, time varying covariates as well as time-constant covariates.

```{r}
e.long<-reshape(data.frame(eclskk5), idvar="childid", varying=list(c("age1","age2"),
                                                     c("age2", "age3"),
                                                    c("povtran1", "povtran2")),
                v.names=c("age_enter", "age_exit","povtrans"),
                times=1:2, direction="long" )
e.long<-e.long[order(e.long$childid, e.long$time),]

#find which kids failed in the first time period and remove them from the second risk period risk set
failed1<-which(is.na(e.long$povtrans)==T)
e.long<-e.long[-failed1,]
e.long$age1r<-round(e.long$age_enter, 0)
e.long$age2r<-round(e.long$age_exit, 0)
head(e.long, n=10)
```

So, this shows us the repeated measures nature of the longitudinal data set.

```{r, fig.width=7, fig.height=6}
#poverty transition .
fit<-survfit(Surv(time = time, event = povtrans)~1, e.long)
fit
summary(fit)
```

This displays the number of poverty transitions between each of the waves.

```{r}
library(survminer)
library(ggplot2)

ggsurvplot(fit, data=e.long, risk.table = T, 
           title="Survival function for poverty transition, K-5th Grade", ylim=c(.9,1))
                                                                                                                 
ggsurvplot(fit, data=e.long, risk.table = T, fun="cumhaz",
           title="Cumulative Hazard function for poverty transition, K-5th Grade")
```
Which, again shows us that at least two of these groups are different from one another.



```{r, fig.height=7, fig.width=8}
library(muhaz)
haz<-kphaz.fit(time=e.long$time, status=e.long$povtrans, method = "product-limit")
haz
kphaz.plot(haz, main="Hazard function plot")
data.frame(haz)
```

This illustrates, that while the largest drop in survivorship occurred between 0 and 1, the hazard is actually higher in the 1-3 month range, illustrating the conditionality of that probability. There is also a large jump in risk at age 1, which may indicate something about age-heaping in the data.

Now we have our S(t) and h(t) functions. We can derive the other functions of survival time from these but integrating (summing) and differentiating these functions. 

```{r, fig.height=7, fig.width=8}
#cumulative hazard
plot(cumsum(haz$haz)~haz$time, 
     main = "Cumulative Hazard function",
     ylab="H(t)",xlab="Time in Months", 
     type="l", lwd=2,col=3)

#Survival function, I just store this in an object so I can use it
surv<-fit

#here is a cheap version of the pdf
ft<- -diff(fit$surv)
plot(ft, 
     type="s",
     ylab="f(t)",xlab="Time in Months",
     main="Probability Density Function")

#here is the cumulative distribution function
Ft<-cumsum(ft)
plot(Ft,  type="s", ylab="F(t)",xlab="Time ", main="Cumulative Distribution Function")
  
```

