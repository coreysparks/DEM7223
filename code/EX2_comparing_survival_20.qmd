---
title: "DEM 7223 - Event History Analysis - Comparing Survival Times Between Groups"
author:
- name: '[Corey S. Sparks, PhD](https://coreysparks.github.io)'
  affiliation: '[The University of Texas at San Antonio](https://hcap.utsa.edu/demography)'
format: 
    html:
      code-fold: true
      code-tools: true
      code-link: true
      df-print: paged
      self-contained: true
editor: visual
---

## Product Limit Estimation

[Kaplan and Meier (1958)](https://www.tandfonline.com/doi/pdf/10.1080/01621459.1958.10501452?casa_token=YIcJDiyQjYwAAAAA:qTa-OQEPtvVy6p4QoBCMx1VIgMey7tJWq-21zMj0LzYKFbKkRO_MYvO1V_7f8qihpgbgllZAa65O) derived an estimator of the survivorship function for a sample of censored and uncensored cases.

-   The method figured survival to a time, t, is the product of the survival from all previous time points.

-   i.e. you can only get to time 3, if you survive time 1 and time 2, etc

We can write this as

$$\hat{S(t)} = (1-\hat{p(t_1)})(1-\hat{p(t_2)}) \dots (1-\hat{p(t_j)})$$

Unlike the life-table and discrete time methods of estimating survival, which lumped time into discrete periods, K-M uses the information contained in the actual duration.

Each K-M interval begins with a single event time, and ends just prior to the next event time.

### Kaplan-Meier Estimation

The K-M estimator is written: $$\hat{S(t)} = \prod_{t_i \leqslant t}\frac{n_i - d_i}{n_i} = \prod_{t_i \leqslant t} \left[1- \frac{d_i}{Y_i} \right]$$

Where the $t_i$ are the ranked survival times, $n_i$ is the number of individuals at risk at each time, $t_i$ is time, $d_i$ is the number of events at each time.

When censoring is present, you define $n_i$ by subtracting out the number of censored cases at that particular time.

```{r, echo=FALSE, message=FALSE}
library(kableExtra)
library(tidyverse)
library(ggsurvfit)
library(gtsummary)
library(survey)
```

```{r, results='asis'}
t1<-data.frame(Time_Interval=c("[0,1)", "[1,2)", "[2,4)", "[4, 5)", "[5, 6)", "[6+)"),
               time=c(0, 1, 2, 4, 5, 6),
               n=c(100,NA, NA, NA, NA, NA),
               d=c(15, 15, 11, 0, 0, 2),
               c=c(0,2,5,2,0,2),
               prob=c(1,NA, NA, NA, NA, NA), 
               St=c(1,NA, NA, NA, NA, NA))

for (i in 2:6){
t1$n[i]<-t1$n[i-1]- t1$d[i-1] -t1$c[i-1]
t1$prob[i]<-1-(t1$d[i]/t1$n[i])
}

t1$St<-cumprod(t1$prob)

t1%>%
  kable(format ="simple" )#%>%
#  column_spec(1:4, border_left = T, border_right = T)#%>%
#  kable_styling()

t1%>%
  ggplot()+
  geom_step(aes(x=time, y=St,color="a"))+
  geom_step(aes(x=time, y=1-prob, color="b"))+
  scale_colour_identity(guide="legend")+
  scale_colour_manual(name = 'Function Type', 
                      values =c('a'='red','b'='green'),
         labels = c('S(t)','p(t)'))+
  xlab("Time")+ylab("Probability")+
  ggtitle("Kaplan - Meier Survival and Probability Functions")
```

### K-M and the hazard function

The exact estimate of the K-M hazard function actually depends on the width of the time interval at each observed time point.

$$\hat{h(t_j)} = \frac{\hat{p_{KM}(t_j)}}{\text{width}_j}$$ and you can get this estimate from the `muhaz` library in R.

### Variance in the K-M Estimates

Since the K-M survival function is a statistical estimate, it also has uncertainty to it. To measure the uncertainty, or variance in the estimate, the traditional method is to use the Greenwood formula.

$$\text{Var}(S(t)) = \hat{S(t)}^2 \sum_{t_i \leqslant t} \frac{d_i}{n_i(n_i-d_i)}$$ and standard error equal to

$s.e.(S(t))= \sqrt{\text{Var}(S(t))}$

If we have a standard error of the survival function, the assuming the sampling distribution of the survival function is normal, we can calculate a normal confidence interval for the survival function at each time:

$$c.i.(S(t)) = \hat{S(t)} \pm z_{1-\alpha/2} * s.e.(S(t))$$

where $z$ is the standard normal variate corresponding to the the $1-\alpha/2$ level of confidence.

#### Estimating the cumulative hazard function.

If we have estimates of $\hat{h(t_j)}$, we can either calculate the value of the cumulative hazard function using the relationship among survival function:

$$H(t) = -\text{log } S(t)$$ or use the Nelson-Aalen estimator:

$$\hat{H(t)} = \sum_{t_i \leqslant t} \frac{d_i}{Y_i}$$

## Comparing Survival curves between groups

Often in our data there are distinct groups for which we want to compare survival patterns \* e.g. treatment vs. control group in clinical trial \* e.g. proportion of women without a first birth by education category

To do this we typically construct a variable that represents an identifier for members of reach group. This can be referred to as an indicator, or class, variable.

This is the same process as doing a two sample test for a regular outcome, such as a t-test or chi square test.

-   One limitation of survival data is that they are typically skewed, meaning their distribution is not symmetrical

-   This means that many traditional hypothesis test (t-test, z-test) for comparison of central tendency are no appropriate

-   Instead we use a variety of non-parametric methods

-   Simply meaning that these tests are not dependent on the shape of the distribution or on the parameters of the distribution

-   Also, due to *censoring*, traditional distributional parameters like the mean are less meaningful, so tests on said parameters would be incorrect

### Graphical methods of comparison

The first stop on our examination of between group comparison is the *inter-ocular traumatic test*

You may not be familiar with this test, but in general, if you look at a plot, and if you think there's a difference, say between two lines, there usually is, and many times the human eye is a more discerning test than anything.

```{r, echo=FALSE}
x<-rnorm(100, mean=10, sd=1)
x2<-rnorm(100, mean=2, sd=2)
df<-data_frame(x=c(x, x2),group=c(rep(1, 100), rep(2, 100)))
df%>%
  ggplot()+geom_density(aes(x, group=group, color=factor(group)))
```

![](https://media.giphy.com/media/GQI382aMVej0k/giphy.gif)

So, plot the survival curves, with confidence intervals, your eye can usually detect if there is a difference

Under traditional statistics thinking, if the confidence intervals of the two curves overlap for their entire lengths, then the two groups are equivalent, if the confidence interval for the curves do not overlap at ANY point along the curve, they are different, simple, no?

The *statistical* way of doing this, beyond looking at things, is the realm of Mantel-Haenszel test. R implements this test for 2 or *k* groups using the `survdiff()` function. It uses the method of [Harrington and Fleming (1982)](https://www.jstor.org/stable/2335991) which can weight the difference in survival curves flexibly, giving more or less weight to earlier or later survival times.

The classic Mantel-Haneszel test is just a $\chi^2$ test for independence.

At each time point, $t_i$, consider the following table for 2 groups:

![M-H Test](C:/Users/ozd504/GitHub/DEM7223/images/mhtable.png)

If we sum the differences between the observed and expected failures across all time points,we arrive with:

Using 1 group as the basis: $$e_{i1} = \frac{n_{i1}d_{i1}}{n_i}$$ is the expected number of failures. The general form of the test is then:

$$Q = \frac{\sum_{i} w_i (d_{i1} - e_{i1})^2}{\sum_i w_i v_{i1}}$$ Where $v_{i1}$ is the variance in the number of events in group 1. This test follows a $\chi^2$ distribution with 1 degree of freedom.

The value of $w_i$ allows great flexibility for these tests, and is called the weight function at time $i$ This allows the analyst to specify how much you want to weight the difference in survival at a particular time point.

This testing logic also extends to *k-groups*, so instead of doing an ANOVA test, you would do the *k-group* test following this method.

## Example

This example will illustrate how to test for differences between survival functions estimated by the Kaplan-Meier product limit estimator. The tests all follow the methods described by Harrington and Fleming (1982) [Link](http://biomet.oxfordjournals.org/content/69/3/553.short).

The first example will use as its outcome variable, the event of a child dying before age 1. The data for this example come from the data [Demographic and Health Survey for 2012](http://dhsprogram.com/data/dataset/model.dat_Standard-DHS_2012.cfm?flag=0) children's recode file. This file contains information for all births in the last 5 years prior to the survey.

The second example, we will examine how to calculate the survival function for a longitudinally collected data set. Here I use data from the [ECLS-K](http://nces.ed.gov/ecls/kinderdatainformation.asp). Specifically, we will examine the transition into poverty between kindergarten and fifth grade.

```{r}
#load libraries
library(haven)
library(survival)
library(car)
library(muhaz)

dat<-read_dta("../data/ZAKR71FL.DTA")
dat<-zap_labels(dat)
```

## Event - Infant Mortality

In the DHS, they record if a child is dead or alive and the age at death if the child is dead. This can be understood using a series of variables about each child.

If the child is alive at the time of interview, then the variable B5==1, and the age at death is censored.

If the age at death is censored, then the age at the date of interview (censored age at death) is the date of the interview - date of birth (in months).

If the child is dead at the time of interview,then the variable B5!=1, then the age at death in months is the variable B7. Here we code this:

```{r}
dat$death.age<-ifelse(dat$b5==1,
                          ((((dat$v008)))-(((dat$b3)))) 
                          ,dat$b7)

#censoring indicator for death by age 1, in months (12 months)
dat$d.event<-ifelse(is.na(dat$b7)==T|dat$b7>12,0,1)
dat$d.eventfac<-factor(dat$d.event)
levels(dat$d.eventfac)<-c("Alive at 1", "Dead by 1")
table(dat$d.eventfac)

```

### Comparing Two Groups

We will now test for differences in survival by characteristics of the household. First we will examine whether the survival chances are the same for children in relatively high ses (in material terms) households, compared to those in relatively low-ses households.

This is the equivalent of doing a t-test, or Mann-Whitney U test for differences between two groups.

```{r}


dat$highses<-Recode(dat$v190,
                    recodes ="1:3 = 0; 4:5=1; else=NA")


fit <-survfit(Surv(death.age, d.event)~highses, data=dat) 

fit%>%
  ggsurvfit() +
  labs(title = "Infant mortality survival plot", subtitle = "By household SES", x = "Years", y = "Survival Probability")+
  xlim(c(0, 12))+
  ylim(c(.95, 1))


summary(fit)

```

Gives us the basic survival plot.

Next we will use `survdiff()` to test for differences between the two or more groups. The `survdiff()` function performs the log-rank test to compare the survival patterns of two or more groups.

```{r}
#two group compairison
survdiff(Surv(death.age, d.event)~highses, data=dat)

```

In this case, we see no difference in survival status based on household SES.

How about rural vs urban residence?

```{r}


dat<-dat%>%
  mutate(rural = car::Recode(v025,
                             recodes ="2 = '0rural'; 1='1urban'; else=NA", as.factor = T))


fit2<-survfit(Surv(death.age, d.event) ~ rural,
              data=dat,
              conf.type = "log")

summary(fit2)

fit2 %>%
  ggsurvfit(xlim=c(0,12),
             ylim=c(.95, 1),
             conf.int=T,
           title="Survival Function for Infant mortality - Rural vs Urban Residence")
```

# Two- sample test

```{r}
survdiff(Surv(death.age, d.event)~rural, data=dat)

prop.table(table(dat$d.event, dat$rural), margin = 2)
chisq.test(table(dat$d.event, dat$rural))
```

Which shows a statistically significant difference in survival between rural and urban children, with rural children showing lower survivorship at all ages.

We can also compare the 95% survival point for rural and urban residents

```{r}
quantile(fit2, probs=.05)

```

### k - sample test

Next we illustrate a k-sample test. This would be the equivalent of the ANOVA if we were doing ordinary linear models.

In this example, I use the `v024` variable, which corresponds to the region of residence in this data. Effectively we are testing for differences in risk of infant mortality by region.

```{r}

table(dat$v024, dat$d.eventfac)

survdiff(Surv(death.age, d.event)~v024, data=dat)

```

Which does not show significant variation in survival between regions.

Lastly, we examine comparing survival across multiple variables, in this case the education of the mother (`secedu`) and the rural/urban residence `rural`:

```{r}

dat<-dat%>%
  mutate(secedu=Recode(v106, recodes ="2:3 = 1; 0:1=0; else=NA"))

table(dat$secedu, dat$d.eventfac)

fit4<-survfit(Surv(death.age, d.event)~rural+secedu, data=dat)

#summary(fit4)

fit4 %>% 
  ggsurvfit(conf.int = F,
             risk.table = F,
           title = "Survivorship Function for Infant Mortality",
           xlab = "Time in Months",
           xlim = c(0,12),
           ylim=c(.9, 1))


# test
survdiff(Surv(death.age, d.event)~rural+secedu, data=dat)

```

Which shows a marginally significant difference between at *least* two of the groups, in this case, I would say that it's most likely finding differences between the Urban, low Education and the Rural high education, because there have the higher ratio of observed vs expected.

# Survival analysis using survey design

This example will cover the use of R functions for analyzing complex survey data. Most social and health surveys are not simple random samples of the population, but instead consist of respondents from a complex survey design. These designs often stratify the population based on one or more characteristics, including geography, race, age, etc. In addition the designs can be multi-stage, meaning that initial strata are created, then respondents are sampled from smaller units within those strata. An example would be if a school district was chosen as a sample strata, and then schools were then chosen as the primary sampling units (PSUs) within the district. From this 2 stage design, we could further sample classrooms within the school (3 stage design) or simply sample students (or whatever our unit of interest is).

A second feature of survey data we often want to account for is differential respondent weighting. This means that each respondent is given a weight to represent how common that particular respondent is within the population. This reflects the differenital probability of sampling based on respondent characteristics. As demographers, we are also often interested in making inference for the population, not just the sample, so our results must be generalizable to the population at large. Sample weights are used in the process as well.

When such data are analyzed, we must take into account this nesting structure (sample design) as well as the respondent sample weight in order to make valid estimates of **ANY** statistical parameter. If we do not account for design, the parameter standard errors will be incorrect, and if we do not account for weighting, the parameters themselves will be incorrect and biased.

In general there are typically three things we need to find in our survey data code books: The sample strata identifier, the sample primary sampling unit identifier (often called a cluster identifier) and the respondent survey weight. These will typically have one of these names and should be easily identifiable in the code book.

Statistical software will have special routines for analyzing these types of data and you must be aware that the diversity of statistical routines that generally exists will be lower for analyzing complex survey data, and some forms of analysis *may not be available!*

In the DHS [Recode manual](http://dhsprogram.com/pubs/pdf/DHSG4/Recode6_DHS_22March2013_DHSG4.pdf), the sampling information for the data is found in variables `v021` and `v022`, which are the primary sampling unit (PSU) and sample strata, respectively. The person weight is found in variable `v005`, and following DHS protocol, this has six implied decimal places, so we must divide it by 1000000, again, following the DHS manual.

```{r}
dat$wt<-dat$v005/1000000

#create the design: ids == PSU, strata==strata, weights==weights.
options(survey.lonely.psu = "adjust")
des<-svydesign(ids=~v021,
               strata = ~v022,
               weights=~wt,
               data=dat)

fit.s<-svykm(Surv(death.age, d.event)~rural,
             design=des, se=T)

#use svyby to find the %of infants that die before age 1, by rural/urban status
svyby(~d.event, ~rural, des, svymean)
```

The plotting is a bit more of a challenge, as the survey version of the function isn't as nice

```{r}

plot(fit.s[[2]], ylim=c(.95,1), xlim=c(0,12),col=1, ci=F )
lines(fit.s[[1]], col=2) 
title(main="Survival Function for Infant Mortality", sub="Rural vs Urban Residence")
legend("topright", legend = c("Urban","Rural" ), col=c(1,2), lty=1)

#test statistic
svylogrank(Surv(death.age, d.event)~rural, design=des)
```

And we see the p-value is larger than assuming random sampling.

# Using a longitudinal data source

In this example, we will examine how to calculate the survival function for a longitudinally collected data set. Here I use data from the [ECLS-K](http://nces.ed.gov/ecls/kinderdatainformation.asp).

![](images/paste-2BD22E30.png)

Specifically, we will examine the transition into poverty between kindergarten and third grade.

First we load our data

```{r}
eclskk5<-readRDS("C:/Users/ozd504/OneDrive - University of Texas at San Antonio/classes/dem7223/dem7223_22//data/eclskk5.rds")
names(eclskk5)<-tolower(names(eclskk5))
```

```{r}
#get out only the variables I'm going to use for this example
myvars<-c( "childid","x_chsex_r", "x_raceth_r", "x1kage_r","x4age",
           "x5age", "x6age", "x7age", "x2povty","x4povty_i", "x6povty_i",
           "x8povty_i","x12par1ed_i", "s2_id")
eclskk5<-eclskk5[,myvars]

```

Recode variables:

```{r}
# time varying variables
eclskk5$age_1<-ifelse(eclskk5$x1kage_r==-9, NA, eclskk5$x1kage_r/12)
eclskk5$age_2<-ifelse(eclskk5$x4age==-9, NA, eclskk5$x4age/12)
#for the later waves, the NCES group the ages into ranges of months,
#so 1= <105 months, 2=105 to 108 months. 
#So, I fix the age at the midpoint of the interval they give,
#and make it into years by dividing by 12

eclskk5$age_3<-ifelse(eclskk5$x5age==-9, NA, eclskk5$x5age/12)

eclskk5$pov_1<-ifelse(eclskk5$x2povty==1,1,0)
eclskk5$pov_2<-ifelse(eclskk5$x4povty_i==1,1,0)
eclskk5$pov_3<-ifelse(eclskk5$x6povty_i==1,1,0)


#Time constant variables
#Recode race with white, non Hispanic as reference using dummy vars
eclskk5$race_rec<-Recode (eclskk5$x_raceth_r, recodes="1 = 'nhwhite';2='nhblack';3:4='hispanic';5='nhasian'; 6:8='other';-9=NA", as.factor = T)
eclskk5$male<-Recode(eclskk5$x_chsex_r, recodes="1=1; 2=0; -9=NA")
eclskk5$mlths<-Recode(eclskk5$x12par1ed_i, recodes = "1:2=1; 3:9=0; else = NA")
eclskk5$mgths<-Recode(eclskk5$x12par1ed_i, recodes = "1:3=0; 4:9=1; else =NA") 
```


**NOTE** I need to remove any children who are missing any of the necessary variables, and who are already in poverty in wave 1, because they are not at risk of experiencing **this particular** transition. 

Again, this is called forming the *risk set*

```{r}
eclskk5<-eclskk5 %>% filter(is.na(pov_1)==F &
                  is.na(pov_2)==F &
                  is.na(pov_3)==F & 
                  is.na(age_1)==F &
                  is.na(age_2)==F &
                  is.na(age_3)==F &
                    pov_1!=1)%>%
  mutate(povtran_1 =ifelse(pov_1==0 & pov_2==0, 0,1), 
         povtran_2 = ifelse(povtran_1==1, NA,
                         ifelse(pov_2==0 & pov_3==0,0,1)))
                         


```

Now we do the entire data set. To analyze data longitudinally, we need to reshape the data from the current "wide" format (repeated measures in columns) to a "long" format (repeated observations in rows). 

The `pivot_long()` function allows us to do this easily. It allows us to specify our repeated measures, time varying covariates as well as time-constant covariates.

```{r}

e.long1 <- eclskk5 %>%
  #rename(wt = w4c4p_40,strata= w4c4p_4str, psu = w4c4p_4psu)%>%
  select(childid,male, race_rec, mlths, mgths,   #time constant
        age_1, age_2, age_3, #t-varying variables
         pov_1, pov_2, pov_3)%>%
   pivot_longer(cols = c(-childid, -male, -race_rec, -mlths, -mgths), #time constant variables go here
               names_to  = c(".value", "wave"), #make wave variable and put t-v vars into columns
               names_sep = "_") #all t-v variables have _ between name and time, like age_1, age_2
```

Now, I need to form the transition variable, this is my event variable, and in this case it will be 1 if a child enters poverty between the first wave of the data and the third grade wave, and 0 otherwise.


```{r}
e.long1 <- e.long1%>%
  group_by(childid)%>%
  mutate(nexpov = dplyr::lead(pov,n=1, order_by = childid))%>%
  mutate(povtran = ifelse(nexpov == 1 & pov == 0, 1, 0))




#find which kids failed in the first time period and remove them from the second risk period risk set
failed1<-which(is.na(e.long1$povtran)==T)
e.long1<-e.long1[-failed1,]

print(e.long1, n = 27)


```

So, this shows us the repeated measures nature of the longitudinal data set.

```{r, fig.width=7, fig.height=6}
#poverty transition
fit<-survfit(Surv(time = as.numeric(wave), event = povtran) ~ 1,
             data = e.long1)
fit
summary(fit)

fit %>%
  ggsurvfit(risk.table = T) +
  labs(title="Survival function for poverty transition, K-5th Grade")
  
```

This displays the number of poverty transitions between each of the waves.

A comparison between groups, based on mom's education level can be done with `survtest()`

```{r}
fit2<- survfit(Surv(time = as.numeric(wave), event = povtran) ~ mlths,
             data = e.long1)

summary(fit2)

fit2 %>%
  ggsurvfit(risk.table = T,
             ylim = c(.5, 1)) +
  labs(title="Survival function for poverty transition, K-5th Grade", 
       subtitle ="By mother's education" )
```

```{r}
survdiff(Surv(time = as.numeric(wave), event = povtran) ~ mlths,
             data = e.long1)
```
